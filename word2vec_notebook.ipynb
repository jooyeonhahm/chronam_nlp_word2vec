{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dcefb47-9cb8-4743-8616-3a435c34308e",
   "metadata": {},
   "source": [
    "# WORD2VEC Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f7f71a-5286-4d6c-a6a4-64e1b7a9d76a",
   "metadata": {},
   "source": [
    "## Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "712d10fd-03b1-4b1c-9ffb-3ada38eb5a8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a92e08c-8ca8-4fd7-b847-12b2bdd4db70",
   "metadata": {},
   "source": [
    "## Load a csv file and text files onto a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bb65278-9a3b-4546-9514-75900fd953b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the year that you would like to conduct data analysis\n",
    "year = 1940"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bc588e9-5718-407f-8078-0f6075579e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata.csv as a Pandas dataframe\n",
    "df = pd.read_csv(f'{year}/metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d02c454-c8d0-4895-941f-ffdf31e75c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A fuction to load .txt files from ocr_texts directory\n",
    "def read_text_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    except FileNotFoundError:\n",
    "        return None  # or return an appropriate response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32d7a8ad-5cc4-4b3f-be6a-f8a0cec183c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the file path is in the 'file' column\n",
    "df['text'] = df['file'].apply(lambda x: read_text_file(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2fd2e0-b534-474b-bec5-9ee5d7e72dbe",
   "metadata": {},
   "source": [
    "## Pre-process the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9c329b",
   "metadata": {},
   "source": [
    "In order to identify frequently misspelled words and context-specific synonyms of your target word, run the sections \"Train Word2Vec\" and \"Analyze Results\" first and compile the list of frequent errors from model.wv.most_similar(positive=['YOUR_TARGET_WORD'], topn=30)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebde2558-ff8a-473a-bfae-22ad62a65c1b",
   "metadata": {},
   "source": [
    "### Improve OCR text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56d2f7b6-2c15-4ded-b775-fca8988b988f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ocr_corrections = {\n",
    "    # Incorrectly splited words\n",
    "    'c hinese': 'chinese',\n",
    "    'ch inese': 'chinese',\n",
    "    'chi nese': 'chinese',\n",
    "    'chin ese': 'chinese',\n",
    "    'chine se': 'chinese',\n",
    "    'chines e': 'chinese',\n",
    "    'ja panese': 'japanese',\n",
    "    'jap anese': 'japanese',\n",
    "    'japa nese': 'japanese',\n",
    "    'japan ese': 'japanse',\n",
    "    'japane se': 'japanese',\n",
    "    'for eign': 'foreign',\n",
    "    'manchu ria': 'manchuria',\n",
    "    'immi grant': 'immigrant',\n",
    "    'immi gration': 'immigration',\n",
    "    # Incorrectly spelled words\n",
    "    'japanse': 'japanese',\n",
    "    # Context-specific synonyms\n",
    "    'nipponese': 'japanese'\n",
    "}\n",
    "\n",
    "def correct_ocr(text, corrections):\n",
    "    for error, correction in corrections.items():\n",
    "        text = text.replace(error, correction)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca8f765c-3f87-49b8-9b80-8f300863e00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "     # Check if the text is a string\n",
    "    if not isinstance(text, str):\n",
    "        # Handle non-string text, e.g., by converting to string or returning a default value\n",
    "        text = str(text)  # or return a default value like ''\n",
    "        \n",
    "    # Remove numbers and special characters from the text\n",
    "    text = re.sub(\"[^A-Za-z]+\", \" \", text)\n",
    "    # Turn all words into lower case\n",
    "    text = text.lower()\n",
    "    # Correct incorrectly split words in the above dictionary\n",
    "    text = correct_ocr(text, ocr_corrections)\n",
    "    return text\n",
    "    \n",
    "df['processed_text'] = df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9b755a-024d-4bc7-9759-d920cb167603",
   "metadata": {},
   "source": [
    "### Tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02762cc0-a70c-4c1b-a743-ad252a49c01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialze lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "english_stopwords = set(stopwords.words('english'))  # Convert to a set for faster membership checking\n",
    "\n",
    "def tokenize_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Lemmetize the text\n",
    "    processed_tokens = [lemmatizer.lemmatize(word) for word in tokens if word.lower() not in english_stopwords]\n",
    "    return processed_tokens\n",
    "\n",
    "df['tokenized_text'] = df['processed_text'].apply(tokenize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6b2e4d-3c21-46db-ae9a-d9aebe78d341",
   "metadata": {},
   "source": [
    "## Train Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b7ad2fd-3167-4f57-b923-9bbecb827c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters (vector_size = 100, window = 20, min_count = 20)\n",
    "model = Word2Vec(df['tokenized_text'], vector_size=100, window=20, min_count=20, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb68d549-469a-4688-83db-7c631b669d3a",
   "metadata": {},
   "source": [
    "## Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6152cbea-34e0-4e2f-8e71-851de0f9c791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('shanghai', 0.7499285936355591),\n",
       " ('jap', 0.723640501499176),\n",
       " ('japan', 0.7170374393463135),\n",
       " ('hai', 0.7092571258544922),\n",
       " ('hongkong', 0.6974321007728577),\n",
       " ('shang', 0.6955792903900146),\n",
       " ('tokyo', 0.6910163164138794),\n",
       " ('orient', 0.6898350715637207),\n",
       " ('tientsin', 0.6875103116035461),\n",
       " ('tokio', 0.674040675163269),\n",
       " ('yangtze', 0.6640757322311401),\n",
       " ('incident', 0.6627698540687561),\n",
       " ('hankow', 0.6591790318489075),\n",
       " ('korea', 0.6585856676101685),\n",
       " ('peiping', 0.646751344203949),\n",
       " ('nippon', 0.6459189653396606),\n",
       " ('chinese', 0.6426478624343872),\n",
       " ('nese', 0.6389297842979431),\n",
       " ('filipino', 0.6320447325706482),\n",
       " ('yokohama', 0.6319870352745056),\n",
       " ('china', 0.6309860944747925),\n",
       " ('abend', 0.6272905468940735),\n",
       " ('manchuria', 0.6264177560806274),\n",
       " ('chlna', 0.624900758266449),\n",
       " ('chungking', 0.6132591366767883),\n",
       " ('militarist', 0.6098368763923645),\n",
       " ('puppet', 0.6091718077659607),\n",
       " ('nanking', 0.6083692312240601),\n",
       " ('domei', 0.6013432145118713),\n",
       " ('manchoukuo', 0.6012725234031677)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change 'japanese' to your target word.\n",
    "model.wv.most_similar(positive=['japanese'], topn=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72f573a4-4802-489e-8cc7-967e43334fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1410837\n"
     ]
    }
   ],
   "source": [
    "enemy = model.wv.similarity('japanese', 'enemy')\n",
    "print(enemy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2acd27d3-b160-40db-9622-8212452d2c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23920898\n"
     ]
    }
   ],
   "source": [
    "immigrant = model.wv.similarity('japanese', 'immigrant')\n",
    "print(immigrant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb638509-c009-47db-9f81-5c7bd75f90f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10520072\n"
     ]
    }
   ],
   "source": [
    "soldier = model.wv.similarity('japanese', 'soldier')\n",
    "print(soldier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea85af8f-bfb4-4ade-916e-43e1b43cb53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005692895\n"
     ]
    }
   ],
   "source": [
    "ally = model.wv.similarity('japanese', 'ally')\n",
    "print(ally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6393ad7e-9f4d-4b1a-a1fb-d09659ea0494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17709936\n"
     ]
    }
   ],
   "source": [
    "american = model.wv.similarity('japanese', 'american')\n",
    "print(american)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a4479d6-4b8b-4854-b3f8-c0e831c3c5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.64264786\n"
     ]
    }
   ],
   "source": [
    "chinese = model.wv.similarity('japanese', 'chinese')\n",
    "print(chinese)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df8755e-2567-4a88-ba3c-d7c7459023eb",
   "metadata": {},
   "source": [
    "## Sentiment Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0034926c-7f52-4f8c-abed-94a0e3d32a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/hahm/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4edac259-19ae-487e-8e0d-35279d57a8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4113111053984594\n"
     ]
    }
   ],
   "source": [
    "def filter_sentences(df, keyword):\n",
    "    # Filter sentences containing the keyword\n",
    "    return df[df['processed_text'].str.contains(keyword, case=False)]\n",
    "\n",
    "def analyze_sentiment(sentences):\n",
    "    # Analyze sentiment of each sentence\n",
    "    sentiment_scores = [sia.polarity_scores(sentence) for sentence in sentences]\n",
    "    return sentiment_scores\n",
    "\n",
    "# Filter sentences\n",
    "japanese_sentences = filter_sentences(df, 'japanese')\n",
    "\n",
    "# Analyze sentiment\n",
    "japanese_sentiment = analyze_sentiment(japanese_sentences['processed_text'])\n",
    "\n",
    "# Summing up all the compound scores\n",
    "j_total_compound_score = sum([sentiment['compound'] for sentiment in japanese_sentiment])\n",
    "\n",
    "# Calculating the average compound score\n",
    "j_average_compound_score = j_total_compound_score / len(japanese_sentiment)\n",
    "\n",
    "print(j_average_compound_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83e03e3-7424-4f69-8364-5d94b69ad414",
   "metadata": {},
   "source": [
    "## Update the CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4a1595e-94eb-4758-8df8-52451957805b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>enemy</th>\n",
       "      <th>immigrant</th>\n",
       "      <th>soldier</th>\n",
       "      <th>ally</th>\n",
       "      <th>american</th>\n",
       "      <th>chinese</th>\n",
       "      <th>j_average_compound_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1917</td>\n",
       "      <td>-0.069088</td>\n",
       "      <td>0.507748</td>\n",
       "      <td>0.058896</td>\n",
       "      <td>0.053292</td>\n",
       "      <td>0.242649</td>\n",
       "      <td>0.517685</td>\n",
       "      <td>0.716156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1918</td>\n",
       "      <td>0.027670</td>\n",
       "      <td>0.293286</td>\n",
       "      <td>-0.102258</td>\n",
       "      <td>0.111673</td>\n",
       "      <td>0.241416</td>\n",
       "      <td>0.667243</td>\n",
       "      <td>0.435222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1919</td>\n",
       "      <td>0.075328</td>\n",
       "      <td>0.458734</td>\n",
       "      <td>0.176309</td>\n",
       "      <td>-0.041869</td>\n",
       "      <td>0.297431</td>\n",
       "      <td>0.528501</td>\n",
       "      <td>0.627945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1920</td>\n",
       "      <td>0.134512</td>\n",
       "      <td>0.502782</td>\n",
       "      <td>0.143171</td>\n",
       "      <td>0.053065</td>\n",
       "      <td>0.225031</td>\n",
       "      <td>0.470205</td>\n",
       "      <td>0.624098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1921</td>\n",
       "      <td>0.094014</td>\n",
       "      <td>0.392006</td>\n",
       "      <td>0.073137</td>\n",
       "      <td>-0.002816</td>\n",
       "      <td>0.306282</td>\n",
       "      <td>0.424202</td>\n",
       "      <td>0.713038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1922</td>\n",
       "      <td>0.007384</td>\n",
       "      <td>0.162523</td>\n",
       "      <td>0.040070</td>\n",
       "      <td>0.087442</td>\n",
       "      <td>0.172594</td>\n",
       "      <td>0.376771</td>\n",
       "      <td>0.727726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1923</td>\n",
       "      <td>0.064556</td>\n",
       "      <td>0.408621</td>\n",
       "      <td>0.094039</td>\n",
       "      <td>-0.008350</td>\n",
       "      <td>0.245259</td>\n",
       "      <td>0.599330</td>\n",
       "      <td>0.627035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1924</td>\n",
       "      <td>0.102849</td>\n",
       "      <td>0.623948</td>\n",
       "      <td>0.015290</td>\n",
       "      <td>-0.027155</td>\n",
       "      <td>0.375301</td>\n",
       "      <td>0.545300</td>\n",
       "      <td>0.642734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1925</td>\n",
       "      <td>0.027900</td>\n",
       "      <td>0.564351</td>\n",
       "      <td>0.109987</td>\n",
       "      <td>0.076618</td>\n",
       "      <td>0.333300</td>\n",
       "      <td>0.553175</td>\n",
       "      <td>0.515829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1926</td>\n",
       "      <td>0.135841</td>\n",
       "      <td>0.446116</td>\n",
       "      <td>0.220782</td>\n",
       "      <td>0.084134</td>\n",
       "      <td>0.328506</td>\n",
       "      <td>0.547844</td>\n",
       "      <td>0.723613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1927</td>\n",
       "      <td>-0.006223</td>\n",
       "      <td>0.281764</td>\n",
       "      <td>0.147132</td>\n",
       "      <td>0.139273</td>\n",
       "      <td>0.191826</td>\n",
       "      <td>0.507917</td>\n",
       "      <td>0.453443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1928</td>\n",
       "      <td>0.008864</td>\n",
       "      <td>0.335525</td>\n",
       "      <td>0.305012</td>\n",
       "      <td>0.042481</td>\n",
       "      <td>0.199738</td>\n",
       "      <td>0.561530</td>\n",
       "      <td>0.653836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1929</td>\n",
       "      <td>-0.035408</td>\n",
       "      <td>0.332279</td>\n",
       "      <td>0.162608</td>\n",
       "      <td>0.017200</td>\n",
       "      <td>0.261654</td>\n",
       "      <td>0.445290</td>\n",
       "      <td>0.491819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1930</td>\n",
       "      <td>0.018031</td>\n",
       "      <td>0.511105</td>\n",
       "      <td>0.102176</td>\n",
       "      <td>0.082520</td>\n",
       "      <td>0.232205</td>\n",
       "      <td>0.413384</td>\n",
       "      <td>0.551160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1931</td>\n",
       "      <td>0.056618</td>\n",
       "      <td>-0.042221</td>\n",
       "      <td>0.317590</td>\n",
       "      <td>-0.069585</td>\n",
       "      <td>0.093293</td>\n",
       "      <td>0.511028</td>\n",
       "      <td>0.216802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1932</td>\n",
       "      <td>0.245643</td>\n",
       "      <td>0.075429</td>\n",
       "      <td>0.433862</td>\n",
       "      <td>0.134851</td>\n",
       "      <td>0.198050</td>\n",
       "      <td>0.654069</td>\n",
       "      <td>-0.091858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1933</td>\n",
       "      <td>0.262858</td>\n",
       "      <td>0.007376</td>\n",
       "      <td>0.294183</td>\n",
       "      <td>0.058830</td>\n",
       "      <td>0.167137</td>\n",
       "      <td>0.740093</td>\n",
       "      <td>0.208268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1934</td>\n",
       "      <td>0.085515</td>\n",
       "      <td>0.116914</td>\n",
       "      <td>0.199726</td>\n",
       "      <td>0.132138</td>\n",
       "      <td>0.113694</td>\n",
       "      <td>0.535072</td>\n",
       "      <td>0.365148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1935</td>\n",
       "      <td>0.091389</td>\n",
       "      <td>0.117467</td>\n",
       "      <td>0.105236</td>\n",
       "      <td>0.121691</td>\n",
       "      <td>0.179952</td>\n",
       "      <td>0.677638</td>\n",
       "      <td>0.574362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1936</td>\n",
       "      <td>0.221941</td>\n",
       "      <td>0.232980</td>\n",
       "      <td>0.310563</td>\n",
       "      <td>0.133152</td>\n",
       "      <td>0.126974</td>\n",
       "      <td>0.723861</td>\n",
       "      <td>0.555078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1937</td>\n",
       "      <td>0.227067</td>\n",
       "      <td>0.012558</td>\n",
       "      <td>0.313929</td>\n",
       "      <td>0.344815</td>\n",
       "      <td>0.206276</td>\n",
       "      <td>0.641533</td>\n",
       "      <td>-0.067906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1938</td>\n",
       "      <td>0.326347</td>\n",
       "      <td>-0.047412</td>\n",
       "      <td>0.358672</td>\n",
       "      <td>0.029810</td>\n",
       "      <td>0.291275</td>\n",
       "      <td>0.639367</td>\n",
       "      <td>0.116008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1939</td>\n",
       "      <td>0.333921</td>\n",
       "      <td>0.130530</td>\n",
       "      <td>0.196019</td>\n",
       "      <td>-0.001512</td>\n",
       "      <td>0.120557</td>\n",
       "      <td>0.602276</td>\n",
       "      <td>0.501416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1940</td>\n",
       "      <td>0.141084</td>\n",
       "      <td>0.239209</td>\n",
       "      <td>0.105201</td>\n",
       "      <td>0.005693</td>\n",
       "      <td>0.177099</td>\n",
       "      <td>0.642648</td>\n",
       "      <td>0.411311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    year     enemy  immigrant   soldier      ally  american   chinese  \\\n",
       "0   1917 -0.069088   0.507748  0.058896  0.053292  0.242649  0.517685   \n",
       "1   1918  0.027670   0.293286 -0.102258  0.111673  0.241416  0.667243   \n",
       "2   1919  0.075328   0.458734  0.176309 -0.041869  0.297431  0.528501   \n",
       "3   1920  0.134512   0.502782  0.143171  0.053065  0.225031  0.470205   \n",
       "4   1921  0.094014   0.392006  0.073137 -0.002816  0.306282  0.424202   \n",
       "5   1922  0.007384   0.162523  0.040070  0.087442  0.172594  0.376771   \n",
       "6   1923  0.064556   0.408621  0.094039 -0.008350  0.245259  0.599330   \n",
       "7   1924  0.102849   0.623948  0.015290 -0.027155  0.375301  0.545300   \n",
       "8   1925  0.027900   0.564351  0.109987  0.076618  0.333300  0.553175   \n",
       "9   1926  0.135841   0.446116  0.220782  0.084134  0.328506  0.547844   \n",
       "10  1927 -0.006223   0.281764  0.147132  0.139273  0.191826  0.507917   \n",
       "11  1928  0.008864   0.335525  0.305012  0.042481  0.199738  0.561530   \n",
       "12  1929 -0.035408   0.332279  0.162608  0.017200  0.261654  0.445290   \n",
       "13  1930  0.018031   0.511105  0.102176  0.082520  0.232205  0.413384   \n",
       "14  1931  0.056618  -0.042221  0.317590 -0.069585  0.093293  0.511028   \n",
       "15  1932  0.245643   0.075429  0.433862  0.134851  0.198050  0.654069   \n",
       "16  1933  0.262858   0.007376  0.294183  0.058830  0.167137  0.740093   \n",
       "17  1934  0.085515   0.116914  0.199726  0.132138  0.113694  0.535072   \n",
       "18  1935  0.091389   0.117467  0.105236  0.121691  0.179952  0.677638   \n",
       "19  1936  0.221941   0.232980  0.310563  0.133152  0.126974  0.723861   \n",
       "20  1937  0.227067   0.012558  0.313929  0.344815  0.206276  0.641533   \n",
       "21  1938  0.326347  -0.047412  0.358672  0.029810  0.291275  0.639367   \n",
       "22  1939  0.333921   0.130530  0.196019 -0.001512  0.120557  0.602276   \n",
       "23  1940  0.141084   0.239209  0.105201  0.005693  0.177099  0.642648   \n",
       "\n",
       "    j_average_compound_score  \n",
       "0                   0.716156  \n",
       "1                   0.435222  \n",
       "2                   0.627945  \n",
       "3                   0.624098  \n",
       "4                   0.713038  \n",
       "5                   0.727726  \n",
       "6                   0.627035  \n",
       "7                   0.642734  \n",
       "8                   0.515829  \n",
       "9                   0.723613  \n",
       "10                  0.453443  \n",
       "11                  0.653836  \n",
       "12                  0.491819  \n",
       "13                  0.551160  \n",
       "14                  0.216802  \n",
       "15                 -0.091858  \n",
       "16                  0.208268  \n",
       "17                  0.365148  \n",
       "18                  0.574362  \n",
       "19                  0.555078  \n",
       "20                 -0.067906  \n",
       "21                  0.116008  \n",
       "22                  0.501416  \n",
       "23                  0.411311  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data to be added as a new row\n",
    "new_data = {\n",
    "    'year': [year],\n",
    "    'enemy': [enemy],\n",
    "    'immigrant': [immigrant],\n",
    "    'soldier': [soldier],\n",
    "    'ally': [ally],\n",
    "    'american': [american],\n",
    "    'chinese': [chinese],\n",
    "    'j_average_compound_score': [j_average_compound_score]\n",
    "}\n",
    "\n",
    "# File path\n",
    "file_path = 'cosine.csv'\n",
    "new_row = pd.DataFrame(new_data)\n",
    "\n",
    "# Check if the file exists\n",
    "try:\n",
    "    # Read the existing CSV file\n",
    "    df_results = pd.read_csv(file_path)\n",
    "except FileNotFoundError:\n",
    "    # If file does not exist, create a new DataFrame\n",
    "    df_results = pd.DataFrame(columns=new_data.keys())\n",
    "\n",
    "# Append the new data\n",
    "df_results= pd.concat([df_results, new_row], ignore_index=True)\n",
    "\n",
    "# Save the updated DataFrame to CSV\n",
    "df_results.to_csv(file_path, index=False)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45df2158-53f9-4c6d-8a55-fc8d1cc5a391",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
